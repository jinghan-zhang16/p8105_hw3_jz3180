Homework 3
================
Jinghan Zhang
October 6, 2020

## Problem 1

``` r
library(tidyverse)
```

    ## ── Attaching packages ───────────────────── tidyverse 1.3.0 ──

    ## ✓ ggplot2 3.3.2     ✓ purrr   0.3.4
    ## ✓ tibble  3.0.3     ✓ dplyr   1.0.2
    ## ✓ tidyr   1.1.2     ✓ stringr 1.4.0
    ## ✓ readr   1.3.1     ✓ forcats 0.5.0

    ## ── Conflicts ──────────────────────── tidyverse_conflicts() ──
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(p8105.datasets)

knitr::opts_chunk$set(
    fig.width = 6, 
  fig.asp = .6,
  out.width = "90%"
  )
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
  )
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
data("instacart")
```

This dataset contains 1384617 rows and 15 columns. Observations are at
the level of items in orders by user. There are user / order variables –
user ID, order ID, order day, and order hour. There are also item
variables – name, aisle, department, and some numeric codes.

How many aisles, and which aisles are the most items from?

``` r
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Let’s make a plot

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

<img src="p8105_hw3_jz3180_files/figure-gfm/plot aisles and items-1.png" width="90%" />

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables
fruits”. Include the number of times each item is ordered in your
table.

``` r
instacart %>%
  filter(aisle %in% c("baking ingredience", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(rank)
```

    ## # A tibble: 6 x 4
    ## # Groups:   aisle [2]
    ##   aisle                    product_name                                  n  rank
    ##   <chr>                    <chr>                                     <int> <int>
    ## 1 dog food care            Snack Sticks Chicken & Rice Recipe Dog T…    30     1
    ## 2 packaged vegetables fru… Organic Baby Spinach                       9784     1
    ## 3 dog food care            Organix Chicken & Brown Rice Recipe          28     2
    ## 4 packaged vegetables fru… Organic Raspberries                        5546     2
    ## 5 dog food care            Small Dog Biscuits                           26     3
    ## 6 packaged vegetables fru… Organic Blueberries                        4966     3

``` r
  #knitr::kable()
```

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week; format this
table for human readers (i.e. produce a 2 x 7 table).

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

## Problem 2

Load, tidy, and otherwise wrangle the data. Your final dataset should
include all originally observed variables and values; have useful
variable names; include a weekday vs weekend variable; and encode data
with reasonable variable classes.

``` r
library(ggridges)

accel_table = read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() 
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

``` r
accel_table$day = 
  factor(accel_table$day, 
         levels 
            = c("Monday",
                "Tuesday",
                "Wednesday",
                "Thursday",
                "Friday",
                "Saturday",
                "Sunday")
         )

cleaned_accel_table <- accel_table[order(accel_table$week, accel_table$day),] %>%
  mutate(day_type = recode(day,
    "Saturday" = "Weekend",
    "Sunday" = "Weekend",
    .default = "Weekday"
    )) %>%
  relocate(day_type, .after = day)
```

This dataset contains 35 rows and 1444 columns. Observations are
accelerometer data collected over 5 weeks on a 63 year-old male with BMI
25, who was admitted to the Advanced Cardiac Care Center of Columbia
University Medical Center and diagnosed with congestive heart failure
(CHF). The variables are the week, day, day type, and activity counts
for each minute of a 24-hour day starting at midnight.

``` r
aggregated <- cleaned_accel_table %>%
  mutate(total_activity = select(cleaned_accel_table, starts_with("activity_")) %>% rowSums()) %>%
  select(week, day_id, day, day_type, total_activity)
  
plot_table <- cleaned_accel_table %>%
  select(day, day_id, starts_with("activity_")) %>%
  gather(minute, activity, activity_1:activity_1440, factor_key = FALSE) %>%
  mutate(minute = as.numeric((str_replace(minute, "activity_", ""))))
  

ggplot(plot_table, aes(x = minute, y = activity))  + 
  geom_point(aes(color = day))
```

<img src="p8105_hw3_jz3180_files/figure-gfm/analyses for Accel data-1.png" width="90%" />

Initially there are no apparent trends from just the table. But with the
single-panel plot, it appears that there are a few time points where
there is more activity. On Sunday’s, there is an activity peak around
minute 600-750 of the day. And on other days of the week, there is an
activity peak around minute 1100-1300 of the day. This makes sense as
usually people are more active towards the later part of the day,
whereas on weekends they have free time earlier in the day too.

## Problem 3

``` r
library(p8105.datasets)
library(ggplot2)
data("ny_noaa") 
```

This dataset contains 2595176 rows and 7 columns. Observations are from
different data collection sites. The variables are the id of site, date
of collected data, precipitation, snow, snow depth, maximum temperature,
and minimum temperature. Just taking a look at the data, there is a lot
of missing data (NA’s), especially in the more recent dates of the
collection time frame. This could prove concerning for data analysis and
visualization.

``` r
noaa <- ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, c("year", "month", "day"), sep = "-") %>%
  mutate(prcp = as.numeric(prcp)) %>%
  mutate(snow = as.numeric(snow)) %>%
  mutate(snwd = as.numeric(snwd)) %>%
  mutate(tmin = as.numeric(tmin)/10) %>%
  mutate(tmax = as.numeric(tmax)/10) %>%
  arrange(year, month, day)
```

For snowfall, the most commonly observed values are 0 because it usually
doesn’t snow during the year.

``` r
tmax_month <- noaa %>%
  select(-prcp, -snow, -snwd, -tmin) %>%
  spread(day, tmax) %>%
  mutate(avg_tmax = rowMeans(select(., "01":"31"), na.rm = TRUE))

tmax_avg <- tmax_month %>%
  select(id, year, month, avg_tmax)

jan_tmax <- tmax_avg %>%
  filter(month == "01") %>%
  filter(!is.na(avg_tmax)) %>%
  select(id, year, avg_tmax)

jan_tmaxplot = 
  jan_tmax %>%
  ggplot(aes(x=year, y=avg_tmax, group=id)) +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  geom_line(size=0.2, alpha=0.1)
  
jul_tmax <- tmax_avg %>%
  filter(month == "07") %>%
  filter(!is.na(avg_tmax)) %>%
  select(id, year, avg_tmax)

jul_tmaxplot = 
  jul_tmax %>%
  ggplot(aes(x=year, y=avg_tmax, group=id)) +
  theme_bw() +
  theme(panel.grid=element_blank()) +
  geom_line(size=0.2, alpha=0.1)

library(ggpubr)

ggarrange(jan_tmaxplot, jul_tmaxplot, 
          labels = c("January", "July"),
          ncol = 2, nrow = 1)
```

<img src="p8105_hw3_jz3180_files/figure-gfm/plotting for noaa data part one-1.png" width="90%" />

For both January and July, the average max temperature in each station
oscillates throughout the years. However, the amplitude of oscillation
(distance from trough to peak) is much less for July, with no apparent
outliers. In January, there are more stark differences throughout the
years - in particular there are very low values from 1994 and 2004.

``` r
tmax_tmin <- noaa %>%
  select(tmax, tmin) %>%
  filter(!is.na(tmax)) %>%
  filter(!is.na(tmin))

tmax_tminplot = 
  tmax_tmin %>%
  ggplot(aes(x = tmin, y = tmax)) +
  geom_hex()

snow_history <- noaa %>%
  select(year, snow) %>%
  filter(snow > 0) %>%
  filter(snow < 100)

snow_historyplot =
  snow_history %>%
  ggplot(aes(x = year)) +
  stat_count(width = 1)

ggarrange(tmax_tminplot, snow_historyplot, 
          labels = c("tmax vs tmin", "Snowfall Distribution"),
          ncol = 2, nrow = 1)
```

<img src="p8105_hw3_jz3180_files/figure-gfm/plotting for noaa data part two-1.png" width="90%" />
